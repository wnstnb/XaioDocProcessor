{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update edit_df to include real-time suggestions for 'answer'\n",
    "import pandas as pd\n",
    "# import streamlit as st\n",
    "from difflib import SequenceMatcher, get_close_matches\n",
    "from paddleocr import PaddleOCR\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "from rapidfuzz import fuzz\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# import torch.nn.functional as F\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import json\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# Functions\n",
    "def get_matches_with_confidence(word, possibilities, n=3, cutoff=0.6):\n",
    "    \"\"\"\n",
    "    Returns matches from `get_close_matches` with their confidence scores.\n",
    "    \n",
    "    Args:\n",
    "        word (str): The word to match.\n",
    "        possibilities (list of str): A list of potential matches.\n",
    "        n (int): Maximum number of close matches to return.\n",
    "        cutoff (float): Minimum similarity ratio for a match to be included.\n",
    "\n",
    "    Returns:\n",
    "        list of tuples: A list of tuples where each tuple is (match, confidence_score).\n",
    "    \"\"\"\n",
    "    # Generate matches using get_close_matches\n",
    "    matches = get_close_matches(word, possibilities, n=n, cutoff=cutoff)\n",
    "    \n",
    "    # Calculate the confidence score for each match\n",
    "    results = []\n",
    "    for match in matches:\n",
    "        score = SequenceMatcher(None, word, match).ratio()\n",
    "        results.append((match, score))\n",
    "    \n",
    "    return results\n",
    "\n",
    "def calculate_containment_ratio(inner_box, outer_box):\n",
    "    \"\"\"\n",
    "    Calculate the containment ratio of `inner_box` within `outer_box`.\n",
    "\n",
    "    Args:\n",
    "        inner_box (list): [x1, y1, x2, y2] for the inner box.\n",
    "        outer_box (list): [x1, y1, x2, y2] for the outer box.\n",
    "\n",
    "    Returns:\n",
    "        float: Containment ratio between 0 and 1.\n",
    "    \"\"\"\n",
    "    # Extract coordinates\n",
    "    x1_inner, y1_inner, x2_inner, y2_inner = inner_box\n",
    "    x1_outer, y1_outer, x2_outer, y2_outer = outer_box\n",
    "\n",
    "    # Calculate intersection coordinates\n",
    "    inter_x1 = max(x1_inner, x1_outer)\n",
    "    inter_y1 = max(y1_inner, y1_outer)\n",
    "    inter_x2 = min(x2_inner, x2_outer)\n",
    "    inter_y2 = min(y2_inner, y2_outer)\n",
    "\n",
    "    # Compute intersection area\n",
    "    inter_width = max(0, inter_x2 - inter_x1)\n",
    "    inter_height = max(0, inter_y2 - inter_y1)\n",
    "    intersection_area = inter_width * inter_height\n",
    "\n",
    "    # Compute area of the inner box\n",
    "    inner_area = (x2_inner - x1_inner) * (y2_inner - y1_inner)\n",
    "\n",
    "    # Avoid division by zero\n",
    "    if inner_area == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # Containment ratio\n",
    "    return intersection_area / inner_area\n",
    "\n",
    "def find_best_contained_bbox(target_bbox, normalized_bboxes, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Find the best contained bbox in normalized_bboxes based on containment ratio.\n",
    "\n",
    "    Args:\n",
    "        target_bbox (list): [x1, y1, x2, y2] for the target box.\n",
    "        normalized_bboxes (list): List of bounding boxes to compare against.\n",
    "        threshold (float): Minimum containment ratio to consider as a match.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (best_match_index, best_containment_ratio) or (None, 0.0) if no match exceeds the threshold.\n",
    "    \"\"\"\n",
    "    best_match_idx = None\n",
    "    best_ratio = 0.0\n",
    "    scores = []\n",
    "    for idx, bbox in enumerate(normalized_bboxes):\n",
    "        ratio = calculate_containment_ratio(bbox, target_bbox)\n",
    "        scores.append(ratio)\n",
    "        if ratio > best_ratio and ratio >= threshold:\n",
    "            best_ratio = ratio\n",
    "            best_match_idx = idx\n",
    "\n",
    "    return best_match_idx, best_ratio, scores\n",
    "\n",
    "\n",
    "\n",
    "def levenshtein_similarity(word, phrases):\n",
    "    \"\"\"\n",
    "    Calculate normalized Levenshtein similarity between `word` and each phrase in `phrases`.\n",
    "\n",
    "    Args:\n",
    "        word (str): The target word or phrase.\n",
    "        phrases (list of str): List of candidate phrases.\n",
    "\n",
    "    Returns:\n",
    "        list: List of tuples (phrase, similarity_score), sorted by descending similarity.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for phrase in phrases:\n",
    "        # Calculate Levenshtein similarity\n",
    "        score = fuzz.ratio(word.lower(), phrase.lower())\n",
    "        results.append((phrase, score / 100))  # Normalize to 0-1 range\n",
    "\n",
    "    # Sort by descending similarity\n",
    "    return sorted(results, key=lambda x: -x[1])\n",
    "\n",
    "def detect_rotation_angle(image):\n",
    "    \"\"\"Detect rotation angle using text alignment.\"\"\"\n",
    "    # Ensure the input is a NumPy array\n",
    "    if not isinstance(image, np.ndarray):\n",
    "        image = np.array(image)\n",
    "    \n",
    "    # Handle grayscale images\n",
    "    if len(image.shape) == 2:\n",
    "        grayscale = image\n",
    "    else:\n",
    "        grayscale = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    edges = cv2.Canny(grayscale, 50, 150, apertureSize=3)\n",
    "    lines = cv2.HoughLinesP(edges, 1, np.pi / 180, threshold=100, minLineLength=100, maxLineGap=10)\n",
    "    \n",
    "    angles = []\n",
    "    if lines is not None:\n",
    "        for line in lines:\n",
    "            x1, y1, x2, y2 = line[0]\n",
    "            angle = np.degrees(np.arctan2(y2 - y1, x2 - x1))\n",
    "            angles.append(angle)\n",
    "\n",
    "    if angles:\n",
    "        median_angle = np.median(angles)\n",
    "        return median_angle\n",
    "    return 0  # Assume no rotation if no angles detected\n",
    "\n",
    "def find_best_and_all_matching_bboxes(target_bbox, normalized_bboxes, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Find the best and all matching bboxes in normalized_bboxes based on containment ratio.\n",
    "\n",
    "    Args:\n",
    "        target_bbox (list): [x1, y1, x2, y2] for the target box.\n",
    "        normalized_bboxes (list): List of bounding boxes to compare against.\n",
    "        threshold (float): Minimum containment ratio to consider as a match.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - best_match (dict): {'index': best_match_index, 'ratio': best_containment_ratio}.\n",
    "            - all_matches (list of dict): [{'index': idx, 'ratio': ratio} for each match above threshold].\n",
    "    \"\"\"\n",
    "    best_match = {'index': None, 'ratio': 0.0}\n",
    "    all_matches = []\n",
    "\n",
    "    for idx, bbox in enumerate(normalized_bboxes):\n",
    "        ratio = calculate_containment_ratio(bbox, target_bbox)\n",
    "        if ratio >= threshold:\n",
    "            all_matches.append({'index': idx, 'ratio': ratio})\n",
    "            if ratio > best_match['ratio']:\n",
    "                best_match = {'index': idx, 'ratio': ratio}\n",
    "\n",
    "    return best_match, all_matches\n",
    "\n",
    "def calculate_containment_ratio(bbox, target_bbox):\n",
    "    \"\"\"Example logic for containment ratio calculation.\"\"\"\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    tx1, ty1, tx2, ty2 = target_bbox\n",
    "\n",
    "    intersection_x1 = max(x1, tx1)\n",
    "    intersection_y1 = max(y1, ty1)\n",
    "    intersection_x2 = min(x2, tx2)\n",
    "    intersection_y2 = min(y2, ty2)\n",
    "\n",
    "    # Compute area of intersection\n",
    "    intersection_area = max(0, intersection_x2 - intersection_x1) * max(0, intersection_y2 - intersection_y1)\n",
    "\n",
    "    # Compute area of the bbox\n",
    "    bbox_area = (x2 - x1) * (y2 - y1)\n",
    "\n",
    "    # Return containment ratio (intersection over bbox area)\n",
    "    return intersection_area / bbox_area if bbox_area > 0 else 0.0\n",
    "\n",
    "def draw_bboxes(image, bboxes, color=\"red\", width=3):\n",
    "    \"\"\"\n",
    "    Draws bounding boxes on an image.\n",
    "\n",
    "    Args:\n",
    "        image (PIL.Image.Image): The input image.\n",
    "        bboxes (list of lists): List of bounding boxes [[x1, y1, x2, y2], ...].\n",
    "        color (str): Color of the bounding boxes. Default is \"red\".\n",
    "        width (int): Line width for the bounding boxes. Default is 3.\n",
    "\n",
    "    Returns:\n",
    "        PIL.Image.Image: Image with bounding boxes drawn.\n",
    "    \"\"\"\n",
    "    # Make a copy of the image to draw on\n",
    "    draw_image = image.copy()\n",
    "    draw = ImageDraw.Draw(draw_image)\n",
    "    \n",
    "    for bbox in bboxes:\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        # Draw the rectangle\n",
    "        draw.rectangle([x1, y1, x2, y2], outline=color, width=width)\n",
    "    \n",
    "    return draw_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ppocr.utils.logging import get_logger\n",
    "import logging\n",
    "logger = get_logger()\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: False\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "gpu_available  = paddle.device.is_compiled_with_cuda()\n",
    "print(\"GPU available:\", gpu_available)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Snowflake/snowflake-arctic-embed-l-v2.0 were not used when initializing XLMRobertaModel: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "converting pages...: 1it [00:02,  2.40s/it]\n",
      "1it [00:00, 95.12it/s]\n"
     ]
    }
   ],
   "source": [
    "from fast_processor import PDFHandler, ClassifyExtract\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "p = PDFHandler(r'test_pages\\drivers_license_test4.pdf')\n",
    "\n",
    "df_pages = p.df_pages\n",
    "\n",
    "clf_results = []\n",
    "clf_confidence = []\n",
    "extraction_results = []\n",
    "for _, row in tqdm(df_pages.iterrows()):\n",
    "    # fp = rf\"{row['preprocessed']}\"\n",
    "    # print(fp)\n",
    "    c = ClassifyExtract(row)\n",
    "    page_label = c.page_label\n",
    "    page_score = c.page_score\n",
    "    clf_results.append(page_label)\n",
    "    clf_confidence.append(page_score)\n",
    "    if page_label != 'Unknown':\n",
    "        res = c.process_image()\n",
    "        res.index.name = 'key'\n",
    "        res = res.reset_index()\n",
    "        res['page_label'] = page_label\n",
    "        res['page_num'] = row['page_number']\n",
    "        extraction_results.append(res)\n",
    "\n",
    "df_pages['page_label'] = clf_results\n",
    "df_pages['page_score'] = clf_confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_extracted \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextraction_results\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Projects\\form-sage\\.venv310\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:382\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 382\u001b[0m op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[1;32mc:\\Projects\\form-sage\\.venv310\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:445\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverify_integrity \u001b[38;5;241m=\u001b[39m verify_integrity\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;241m=\u001b[39m copy\n\u001b[1;32m--> 445\u001b[0m objs, keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_clean_keys_and_objs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[0;32m    448\u001b[0m ndims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ndims(objs)\n",
      "File \u001b[1;32mc:\\Projects\\form-sage\\.venv310\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:507\u001b[0m, in \u001b[0;36m_Concatenator._clean_keys_and_objs\u001b[1;34m(self, objs, keys)\u001b[0m\n\u001b[0;32m    504\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[0;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs_list) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    510\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs_list))\n",
      "\u001b[1;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "df_extracted = pd.concat(extraction_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.show_bboxes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.df_pages['preprocessed'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fast_processor import ClassifyExtract\n",
    "# image_path = r'debug_images\\1120S_p1_2024_12_18_224445_957\\page_1\\2_denoised.png'\n",
    "# image_path = r'debug_images\\1040_p1_2024_12_27_085517_449\\page_1\\2_denoised.png'\n",
    "# img = Image.open(image_path)\n",
    "t = ClassifyExtract(image_path=p.df_pages['preprocessed'][0])\n",
    "results, page_label, page_score = t.process_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.show_bboxes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.show_bboxes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.show_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.bbox_draw_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fast_classifier import load_template_database, classify_document_with_confidence\n",
    "from datetime import datetime\n",
    "\n",
    "template_db = load_template_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label, score, _, alls = classify_document_with_confidence(image_path=image_path,template_db=template_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr = PaddleOCR(lang=\"en\", cls=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "image_width, image_height = image.size\n",
    "\n",
    "ocr_results = ocr.ocr(image_path)[0]\n",
    "\n",
    "words = [line[1][0] for line in ocr_results]\n",
    "bboxes = []\n",
    "for box in [line[0] for line in ocr_results]:\n",
    "    x1 = min(point[0] for point in box)\n",
    "    y1 = min(point[1] for point in box)\n",
    "    x2 = max(point[0] for point in box)\n",
    "    y2 = max(point[1] for point in box)\n",
    "    bboxes.append([x1, y1, x2, y2])\n",
    "\n",
    "normalized_bboxes = [\n",
    "    [\n",
    "        (x1 / image_width),\n",
    "        (y1 / image_height),\n",
    "        (x2 / image_width),\n",
    "        (y2 / image_height),\n",
    "    ]\n",
    "    for x1, y1, x2, y2 in bboxes\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CUDA availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = 'Snowflake/snowflake-arctic-embed-l-v2.0'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name, add_pooling_layer=False)\n",
    "model.eval().to(device)  # Move model to GPU\n",
    "\n",
    "with open('labels.json', 'r') as f:\n",
    "    k = json.load(f)\n",
    "\n",
    "# Define queries and documents\n",
    "query_prefix = 'query: '\n",
    "keys = [t['key'] for t in k[label]]\n",
    "queries = [t['question'] for t in k[label]]\n",
    "coords = [t['target_coords'] for t in k[label]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get special instructions\n",
    "instructs = []\n",
    "search_areas = []\n",
    "ignore_words = []\n",
    "for t in k[label]:\n",
    "    try:\n",
    "        instructs.append(t['prep'])\n",
    "    except:\n",
    "        instructs.append(None)\n",
    "\n",
    "for t in k[label]:\n",
    "    try:\n",
    "        search_areas.append(t['search_coords'])\n",
    "    except:\n",
    "        search_areas.append(None)\n",
    "\n",
    "for t in k[label]:\n",
    "    try:\n",
    "        ignore_words.append(t['ignore_words'])\n",
    "    except:\n",
    "        ignore_words.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare input tokens\n",
    "queries_with_prefix = [f\"{query_prefix}{i}\" for i in queries]\n",
    "query_tokens = tokenizer(queries_with_prefix, padding=True, truncation=True, return_tensors='pt', max_length=8192).to(device)\n",
    "document_tokens = tokenizer(words, padding=True, truncation=True, return_tensors='pt', max_length=8192).to(device)\n",
    "\n",
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    query_embeddings = model(**query_tokens)[0][:, 0]\n",
    "    document_embeddings = model(**document_tokens)[0][:, 0]\n",
    "\n",
    "# Normalize embeddings\n",
    "query_embeddings = torch.nn.functional.normalize(query_embeddings, p=2, dim=1)\n",
    "document_embeddings = torch.nn.functional.normalize(document_embeddings, p=2, dim=1)\n",
    "\n",
    "# Compute cosine similarity\n",
    "scores = torch.mm(query_embeddings, document_embeddings.transpose(0, 1))\n",
    "\n",
    "normalized_scores = (scores + 1) / 2\n",
    "\n",
    "search_masks = []\n",
    "word_masks = []\n",
    "# Carve out the relevant scores that have bboxes that fall into the search coords\n",
    "for s in search_areas:\n",
    "    if s:\n",
    "        # Get search area\n",
    "        a1, a2, a3, a4 = eval(s)\n",
    "        # Iterate through bboxes\n",
    "        masks = []\n",
    "        for b in normalized_bboxes:\n",
    "            b1, b2, b3, b4 = b\n",
    "            val = 1 if np.all([a1 <= b1, a2 <= b2, a3 >= b3, a4 >= b4]) else 0\n",
    "            masks.append(val)\n",
    "        search_masks.append(masks)\n",
    "    else:\n",
    "        search_masks.append([1 for i in range(len(bboxes))])\n",
    "\n",
    "for s in ignore_words:\n",
    "    if s:\n",
    "        # Get list of words for removal\n",
    "        igs = eval(s)\n",
    "        # Iterate through bboxes\n",
    "        # For each word to ignore,\n",
    "        masks = []\n",
    "        for s in igs:\n",
    "            submask = []\n",
    "            # Iterate through each word in the document\n",
    "            for b in words:\n",
    "                # Assign 0 if a word contains a word to ignore, 1 otherwise\n",
    "                val = 0 if s.lower() in b.lower() else 1\n",
    "                # Append the value to an array\n",
    "                submask.append(val)\n",
    "            masks.append(submask)\n",
    "            # Multiply all arrays in logics so that we end up with the mask for which scores to keep\n",
    "        masks = np.prod(masks, axis=0)\n",
    "        word_masks.append(masks)\n",
    "    else:\n",
    "        word_masks.append([1 for i in range(len(bboxes))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_masks = [np.array(t) * np.array(z) for t, z in zip(search_masks, word_masks)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_scores = [[float(n) for n in s] * np.array(z) for s, z in zip(normalized_scores, all_masks)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_matches = [(words[np.argmax(s).item()], s[np.argmax(s).item()]) for s in filtered_scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_list = []\n",
    "target_bbox_list = []\n",
    "label_data  = {}\n",
    "all_answers = []\n",
    "counter = 0\n",
    "\n",
    "for t in best_matches:\n",
    "    labels = {}\n",
    "    # Match value for the label\n",
    "    match_word = t[0]\n",
    "    # Match confidence for the label\n",
    "    match_confidence = t[1]\n",
    "    # Find the index of the matching word in words\n",
    "    match_idx = [i for i, w in enumerate(words) if w == match_word][0]\n",
    "    # Find the corresponding bbox of the label\n",
    "    match_bbox = normalized_bboxes[match_idx]\n",
    "    # Store label info in dictionary\n",
    "    label_data[keys[counter]] = {}\n",
    "    label_data[keys[counter]]['label'] = match_word\n",
    "    label_data[keys[counter]]['label_bbox'] = match_bbox\n",
    "    label_data[keys[counter]]['label_confidence'] = match_confidence\n",
    "    counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that you have the label data stored, it can be used in reference \n",
    "counter = 0\n",
    "for t, bb in zip(label_data.keys(), coords):\n",
    "    bbox_list.append(label_data[t]['label_bbox'] * np.array([image_width, image_height, image_width, image_height]))\n",
    "    match_bbox = label_data[t]['label_bbox']\n",
    "    x1, y1, x2, y2 = match_bbox\n",
    "    target_bbox = eval(bb)\n",
    "    search_area = np.array(target_bbox) * np.array([image_width, image_height, image_width, image_height])\n",
    "    bbox_list.append(search_area)\n",
    "    best_match, all_matches = find_best_and_all_matching_bboxes(target_bbox, normalized_bboxes, threshold=0.5)\n",
    "    match_words = []\n",
    "    match_scores = []\n",
    "    match_area = []\n",
    "    for i in all_matches:\n",
    "        match_words.append(words[i['index']])\n",
    "        match_scores.append(i['ratio'])\n",
    "        match_area.append(bboxes[i['index']])\n",
    "        bbox_list.append(bboxes[i['index']])\n",
    "\n",
    "    label_data[t]['value'] = match_words #' '.join(match_words)\n",
    "    if instructs[counter]:\n",
    "        if '[' in instructs[counter]:\n",
    "            func = eval(instructs[counter])\n",
    "            label_data[t]['value'] = func(match_words)\n",
    "        else:\n",
    "            label_data[t]['value'] = [eval(instructs[counter])(a) for a in label_data[t]['value']][0]\n",
    "    else:\n",
    "        label_data[t]['value'] = ' '.join(label_data[t]['value'])\n",
    "    label_data[t]['value_bbox'] = [\n",
    "        min(x[0] for x in match_area),  # Min of element 1\n",
    "        min(x[1] for x in match_area),  # Min of element 2\n",
    "        max(x[2] for x in match_area),  # Max of element 3\n",
    "        max(x[3] for x in match_area)   # Max of element 4\n",
    "    ] if match_area else [0,0,0,0]\n",
    "    bbox_list.append(label_data[t]['value_bbox'])\n",
    "    label_data[t]['value_confidence'] = np.mean(match_scores)\n",
    "    # all_answers.append(answers)\n",
    "    counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame.from_dict(label_data,orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# tx1, ty1, tx2, ty2 = target_bbox\n",
    "\n",
    "# bbox_list = [\n",
    "#     bboxes[match_idx],\n",
    "#     bboxes[best_idx] if best_idx else [0,0,0,0],\n",
    "#     [tx1 * image_width, ty1 * image_height, tx2 * image_width, ty2 * image_height]\n",
    "# ]\n",
    "# Draw bounding boxes\n",
    "annotated_image = draw_bboxes(image, bbox_list, color=\"red\", width=4)\n",
    "\n",
    "# Show and save the annotated image\n",
    "annotated_image.show()  # Open in default viewer\n",
    "annotated_image.save(\"annotated_image.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[num for num, w in enumerate(words) if 'assels' in w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz import fuzz, process\n",
    "\n",
    "def robust_fuzzy_matching(word, phrases):\n",
    "    \"\"\"\n",
    "    Robust fuzzy matching using RapidFuzz.\n",
    "    \n",
    "    Args:\n",
    "        word (str): The word/phrase to match.\n",
    "        phrases (list): List of candidate phrases.\n",
    "        \n",
    "    Returns:\n",
    "        list: Sorted list of (phrase, confidence) tuples.\n",
    "    \"\"\"\n",
    "    return process.extract(word, phrases, scorer=fuzz.ratio, limit=3)\n",
    "\n",
    "# Example usage\n",
    "word = \"gross profit\"\n",
    "phrases = [\n",
    "    \"Gross receipts or sales\",\n",
    "    \"Gross profit.Subtract line 2 from line 1c.\",\n",
    "    \"Print\"\n",
    "]\n",
    "results = robust_fuzzy_matching(word, phrases)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_pipeline import layoutlm_paddleocr_pipeline\n",
    "\n",
    "layoutlm_paddleocr_pipeline(image_path=image_path, question='What is gross profit?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the model\n",
    "model_name = 'Snowflake/snowflake-arctic-embed-l-v2.0'\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# Define the queries and documents\n",
    "queries = [\n",
    "    'Locate the line with the word \"Balance\"',\n",
    "    'Locate the cost of goods sold line.',\n",
    "    'Locate the gross profit line.',\n",
    "    'Locate the \"Interest\" line for interest paid.',\n",
    "    'Locate the \"Depreciation\" line',\n",
    "    'Locate the line for total deductions.'\n",
    "    'Locate the line for net profit.'\n",
    "    ]\n",
    "\n",
    "# Compute embeddings: use prompt_name=\"query\" to encode queries!\n",
    "query_embeddings = model.encode(queries, prompt_name=\"query\") \n",
    "document_embeddings = model.encode(words)\n",
    "\n",
    "# Compute cosine similarity scores\n",
    "scores = model.similarity(query_embeddings, document_embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float(best_matches[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = F.softmax(scores, dim=-1)\n",
    "[(words[torch.argmax(s).item()], s[torch.argmax(s).item()]) for s in sm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([words[np.argmax(s)] for s in scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([(words[np.argmax(s)], s[np.argmax(s)]) for s in scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Output the results\n",
    "for query, query_scores in zip(queries, scores):\n",
    "    doc_score_pairs = list(zip(words, query_scores))\n",
    "    doc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\n",
    "    print(\"Query:\", query)\n",
    "    for document, score in doc_score_pairs:\n",
    "        print(score, document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
